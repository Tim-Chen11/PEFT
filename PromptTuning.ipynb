{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a **comprehensive overview** of **Prompt Tuning** under the **Parameter-Efficient Fine-Tuning (PEFT)** umbrella. I’ll explain **what Prompt Tuning is**, **why it’s used**, **how it differs from other PEFT methods** (like LoRA or adapters), and **how you can implement it** in practice.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. What is PEFT?**\n",
    "**Parameter-Efficient Fine-Tuning (PEFT)** is a set of techniques that adapt large language models (LLMs) to downstream tasks **without updating all of the model’s parameters**. Instead, **PEFT methods** add a small number of trainable parameters, significantly reducing the memory footprint and compute cost compared to full fine-tuning.\n",
    "\n",
    "**Popular PEFT methods** include:\n",
    "1. **Prompt Tuning** (and variants like P-Tuning, Prefix Tuning)\n",
    "2. **LoRA** (Low-Rank Adaptation)\n",
    "3. **Adapters**\n",
    "4. **BitFit** (tuning only bias terms)\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Prompt Tuning in a Nutshell**\n",
    "**Prompt Tuning** introduces **learnable “prompt tokens”** (or “prompt embeddings”) that you prepend (or insert) into the input sequence. **All original model parameters are frozen**, and **only the newly introduced prompt embeddings** are trained. This approach teaches the model how to interpret a small set of extra tokens in a way that steers the model toward your downstream task.\n",
    "\n",
    "### **2.1 How It Works**\n",
    "1. **Frozen LLM**: Start with a large pretrained model (e.g., GPT-2, T5, LLaMA). **Do not update** its internal weights.\n",
    "2. **Add Prompt Embeddings**: Introduce a small set of **prompt tokens** (e.g., 20–100 tokens) that have **trainable embeddings**.  \n",
    "   - In text form, you can think of them as special tokens: `[PROMPT_1]`, `[PROMPT_2]`, etc.  \n",
    "   - Internally, each special token has its own embedding vector.\n",
    "3. **Concatenate Prompt Tokens + Input**: For every training example, your actual text is preceded (or appended) by these prompt tokens.  \n",
    "   - Example (simplified):  \n",
    "     `[PROMPT_1] [PROMPT_2] ... [PROMPT_N] + \"Question: What is the diagnosis?\"`\n",
    "4. **Forward Pass**: The model sees the prompt tokens + user text. Because the **prompt embeddings are trainable**, the backpropagation updates only those embedding vectors.\n",
    "5. **Adaptation**: Over training steps, the prompt embeddings learn how to **guide the frozen LLM** to produce correct outputs for your task (classification, QA, text generation, etc.).\n",
    "\n",
    "### **2.2 Why Prompt Tuning?**\n",
    "- **Parameter Efficiency**: Only a small set of embedding vectors (the “prompt embeddings”) are updated.  \n",
    "- **Memory Efficiency**: No need to load or modify the entire model.  \n",
    "- **Task-Specific**: You can create multiple sets of prompt embeddings for different tasks, while still using the same base LLM.  \n",
    "- **Comparable Performance**: In many tasks, prompt tuning can achieve performance close to full fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Variants of Prompt Tuning**\n",
    "1. **Prefix Tuning**: Similar idea, but instead of adding tokens at the embedding layer, you insert “prefix activations” at each layer of the Transformer.  \n",
    "2. **P-Tuning / P-Tuning v2**: Extends prefix tuning with deeper prompt parameters for each layer, often used with GPT-like models.  \n",
    "3. **Soft Prompt Tuning** (another term): Same concept—trainable prompt embeddings that are not discrete tokens but learnable vectors.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Prompt Tuning vs. Other PEFT Methods**\n",
    "\n",
    "| **Method**     | **Key Idea**                                       | **Trainable Parameters**        | **Pros**                                     | **Cons**                                            |\n",
    "|----------------|----------------------------------------------------|--------------------------------|----------------------------------------------|-----------------------------------------------------|\n",
    "| **Prompt Tuning** | Add learnable tokens at input level               | Embeddings for new prompt tokens | Very low overhead; easy to swap prompts       | May not always match full fine-tuning performance   |\n",
    "| **LoRA**       | Insert low-rank matrices into transformer layers    | Low-rank weight updates         | Great performance–efficiency tradeoff         | More code changes than simple prompt tuning         |\n",
    "| **Adapters**   | Insert small MLP layers (“adapters”) in each layer  | Adapter parameters              | Good for complex tasks, modular design        | Slightly larger overhead than prompt tuning/LoRA    |\n",
    "| **BitFit**     | Only tune bias terms                                | Bias terms in each layer        | Extremely simple to implement                | Performance can be lower than LoRA/prompt tuning    |\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Detailed Steps to Implement Prompt Tuning**\n",
    "\n",
    "Let’s assume you’re using the **Hugging Face Transformers** ecosystem plus the **PEFT** library.\n",
    "\n",
    "### **5.1 Installation**\n",
    "```bash\n",
    "pip install transformers peft accelerate\n",
    "```\n",
    "- `transformers` → for the base model.  \n",
    "- `peft` → official library from Hugging Face implementing PEFT methods (Prompt Tuning, LoRA, etc.).  \n",
    "- `accelerate` → for efficient training on multi-GPU setups.\n",
    "\n",
    "### **5.2 Load a Pretrained Model and Tokenizer**\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PromptTuningConfig, get_peft_model\n",
    "\n",
    "model_name = \"gpt2\"  # or \"facebook/opt-1.3b\", \"google/flan-t5-base\", etc.\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "```\n",
    "- Here we’re using a **causal LM** (GPT-2) for demonstration.  \n",
    "- For T5 or other encoder-decoder models, the steps are similar.\n",
    "\n",
    "### **5.3 Create a Prompt Tuning Configuration**\n",
    "```python\n",
    "from peft import PromptTuningInit, PromptTuningConfig\n",
    "\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=\"CAUSAL_LM\",        # or \"SEQ_2_SEQ_LM\", \"TOKEN_CLS\", etc.\n",
    "    prompt_length=20,             # number of prompt tokens\n",
    "    init_from_text=\"This is a medical prompt:\",  # optional initialization\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,    # how to init the prompt embeddings\n",
    "    num_virtual_tokens=20,        # same as prompt_length\n",
    ")\n",
    "```\n",
    "- **prompt_length**: The number of special tokens to prepend.  \n",
    "- **init_from_text** (optional): You can initialize prompt embeddings from real text (like “This is a medical prompt:”). The library extracts the embedding from that text and uses it as a starting point.\n",
    "\n",
    "### **5.4 Wrap the Base Model with PEFT**\n",
    "```python\n",
    "peft_model = get_peft_model(base_model, peft_config)\n",
    "```\n",
    "- This adds a “prompt embedding” table inside your model.  \n",
    "- All other weights in `base_model` remain **frozen**.\n",
    "\n",
    "### **5.5 Training Loop**\n",
    "```python\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    learning_rate=1e-4,\n",
    ")\n",
    "\n",
    "# Suppose you have a dataset of (input, labels) pairs\n",
    "# For causal LM, your dataset might look like:\n",
    "# [\n",
    "#   {\"input_ids\": [...], \"labels\": [...]},\n",
    "#   ...\n",
    "# ]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=train_args,\n",
    "    train_dataset=your_train_dataset,\n",
    "    eval_dataset=your_eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n",
    "- During training, **only the prompt embeddings** get updated.  \n",
    "- **Memory usage** is much lower than full fine-tuning.  \n",
    "- **Performance** often matches or approaches full fine-tuning if the task is well-defined.\n",
    "\n",
    "### **5.6 Inference**\n",
    "After training, you can do:\n",
    "```python\n",
    "prompt_text = \"Patient complains of headache and nausea. Possible diagnosis?\"\n",
    "input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids\n",
    "outputs = peft_model.generate(input_ids, max_length=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "- The prompt embeddings automatically get prepended (under the hood) for each forward pass.  \n",
    "- You can store and reuse these prompt embeddings for different tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Use Cases & Best Practices**\n",
    "\n",
    "### **6.1 Use Cases**\n",
    "1. **Domain Adaptation**: Medical, legal, or scientific text → Prompt Tuning can incorporate domain style or knowledge.  \n",
    "2. **Classification or QA**: Freeze the main model and learn only prompt tokens that help in classification tasks.  \n",
    "3. **Few-Shot Learning**: A small labeled dataset can still produce good results with prompt tuning.\n",
    "\n",
    "### **6.2 Best Practices**\n",
    "1. **Prompt Length**: 10–50 tokens is typical. Too many tokens can cause confusion; too few might limit capacity.  \n",
    "2. **Initialization**: If possible, **initialize from meaningful text** to speed up convergence.  \n",
    "3. **Regularization**: Sometimes you can apply dropout or weight decay to the prompt embeddings.  \n",
    "4. **Evaluation**: Compare **prompt tuning** results to **full fine-tuning** or **LoRA** to see if you’re hitting your performance goals.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Advantages and Limitations**\n",
    "\n",
    "### **7.1 Advantages**\n",
    "- **Highly Parameter-Efficient**: Only store & update a small fraction of model parameters.  \n",
    "- **Easy Model Management**: You can keep a single large model checkpoint and swap out prompt “modules” for different tasks.  \n",
    "- **Reduced Risk of Overfitting**: Since the main model is frozen, you’re less likely to overfit on small data.\n",
    "\n",
    "### **7.2 Limitations**\n",
    "- **Task Sensitivity**: Some tasks might require deeper modifications (LoRA or Adapters) to achieve top performance.  \n",
    "- **Limited Expressiveness**: You only control the initial prompt embeddings. If your downstream task is drastically different from the model’s pretraining domain, performance might suffer.  \n",
    "- **Initialization**: Poor initialization of prompt embeddings can lead to slow or unstable training.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Summary**\n",
    "- **Prompt Tuning** is a **PEFT** method that **freezes** all original LLM parameters and **only learns** a small set of **prompt embeddings**.  \n",
    "- It’s extremely **memory-efficient**, often achieving **near-full-fine-tuning** performance with **far fewer trainable parameters**.  \n",
    "- **Implementation** in **Hugging Face PEFT** is straightforward: define a `PromptTuningConfig`, wrap the model, and run your standard training loop.  \n",
    "- Ideal for **domain adaptation** (like medical or legal) and for organizations that want to maintain a single large model while creating multiple domain/task-specific “prompt modules.”\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**  \n",
    "1. **Prompt Tuning** is part of **PEFT**, focusing on **minimal parameter updates** (prompt embeddings).  \n",
    "2. It’s best for tasks where the **frozen LLM** already has strong capabilities.  \n",
    "3. If you need **more capacity** or deeper changes, consider **LoRA** or **adapter** approaches.  \n",
    "\n",
    "**Prompt Tuning** is an exciting and **practical** way to harness the power of massive LLMs while keeping your training footprint small—perfect for scenarios with limited computational resources or the need to manage **multiple tasks** on top of a single foundation model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
